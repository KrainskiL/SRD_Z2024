{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2 - Classifiers evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"] = 120.0  # size of figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Credit - Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric\"\n",
    "DATA_SET = pd.read_fwf(url, header=None)\n",
    "DATA_SET.rename(columns={24: \"target\"}, inplace=True)\n",
    "DATA_SET[\"target\"] = DATA_SET[\"target\"] - 1  # recoding target variable\n",
    "DATA_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = DATA_SET.drop([\"target\"], axis=1)\n",
    "y = DATA_SET[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is random sampling the best approach? What if one class has many more records than the other?\n",
    "Imbalanced data may lead to poor model which may have good overall performance metrics e.g. accuracy.\n",
    "\n",
    "There are several approaches to tackle the issue:\n",
    "- undersampling, \n",
    "- oversampling, \n",
    "- **cost-based analysis**,\n",
    "- algorithmic approches e.g. SMOTE\n",
    "\n",
    "Check [imbalanced-learn](https://imbalanced-learn.org/stable/user_guide.html#user-guide) documentation for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Building logistic regression model](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None, max_iter=1000)\n",
    "LR = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.intercept_, LR.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict probability of bad credit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On validation data\n",
    "score_val = LR.predict_proba(X_test)[:, 1]\n",
    "# On training data\n",
    "score_train = LR.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "‚ùó Remember class indicator (0, 1,...) and actual or predicted values may be switched in confusion matrix\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" width=400>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1780/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\"  width=400>\n",
    "\n",
    "**Performance measures derived from confusion matrix:**\n",
    "\n",
    "- Accuracy - percentage of correct predictions\n",
    "\n",
    "`ACC = (TP + TN)/(TP + FP + TN + FN)`\n",
    "\n",
    "- Precision - percentage of positive predictions which were actually correct\n",
    "\n",
    "`PREC = TP / (TP + FP)`\n",
    "\n",
    "-  Recall - what percentage of actual positives were predicted correctly\n",
    " (Recall = Sensitivity = Hit rate = True Positive Rate (TPR))\n",
    " \n",
    "`REC = TP / (TP + FN)`\n",
    "\n",
    "- F1 Score - traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1bf179c30b00db201ce1895d88fe2915d58e6bfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-based approach in model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_analysis(\n",
    "    y_test: pd.Series, y_test_hat: pd.Series, cost_matrix: np.array\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Calculate cost for cutoff thresholds between 0 and 1 for given true labels `y_test`,\n",
    "    predicted labels `y_test_hat` and cost matrix `cost_matrix`.\n",
    "    \"\"\"\n",
    "    cutoff_range = np.arange(0, 1.0, 0.01)\n",
    "    vec = []\n",
    "    for cutoff in cutoff_range:\n",
    "        y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "        conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "        conf_const_mat = np.multiply(conf_mat, cost_matrix)\n",
    "        vec.append(conf_const_mat.sum() / len(y_test))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costmat = np.array([[0, 1], [5, 0]])\n",
    "cost_val = cutoff_analysis(y_test, score_val, costmat)\n",
    "cost_train = cutoff_analysis(y_train, score_train, costmat)\n",
    "cost_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Cutoff point\")\n",
    "plt.ylabel(\"Cost per client\")\n",
    "plt.title(\"Cost vs. cutoff threshold\")\n",
    "\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_val, color=\"blue\")\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_train, color=\"red\")\n",
    "cutoffs = np.arange(0, 1.0, 0.01)\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [min(cost_val)] * 2,\n",
    "    color=\"blue\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"Min Cost Val = {min(cost_val):.2f} for k = {cutoffs[np.argmin(cost_val)]}\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [min(cost_train)] * 2,\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"Min Cost Train = {min(cost_train):.2f} for k = {cutoffs[np.argmin(cost_train)]}\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got lower cost for predictions on training set - model may **overfit** slightly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat\"\n",
    "dataset = pd.read_csv(\n",
    "    url, sep=\" \", header=None, names=[\"V\" + str(i) for i in range(0, 15)]\n",
    ")\n",
    "# dataset.columns = [\"V\" + str(i) for i in range(0,15)]\n",
    "dataset.rename(columns={\"V14\": \"class\"}, inplace=True)\n",
    "\n",
    "dataset[\"V3\"] = np.where(dataset[\"V3\"] == 1, 0, 1)\n",
    "dataset[\"V11\"] = np.where(dataset[\"V11\"] == 1, 0, 1)\n",
    "dataset[\"V13\"] = np.log(dataset[\"V13\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fraction = 0.8\n",
    "X = dataset.iloc[:, 0:14]\n",
    "y = dataset.iloc[:, 14]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1 - training_fraction, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lr(penalty=None, max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is what in confusion matrix?\n",
    "confm = confusion_matrix(y_test, y_test_hat)\n",
    "confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC = (confm[0, 0] + confm[1, 1]) / np.sum(confm)\n",
    "PREC = (confm[1, 1]) / (confm[1, 1] + confm[0, 1])\n",
    "REC = (confm[1, 1]) / (confm[1, 1] + confm[1, 0])\n",
    "F1 = 2 * PREC * REC / (PREC + REC)\n",
    "print(\"ACC \", ACC, \"\\nPREC \", PREC, \"\\nREC \", REC, \"\\nF1 \", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sklearn built-in report\n",
    "print(classification_report(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual performance metric - ROC Curve + descriptive AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = model2.predict_proba(X_train)[:,1]\n",
    "y_test_hat = model2.predict_proba(X_test)[:,1]\n",
    "fprv, tprv, _ = roc_curve(y_test, y_test_hat)\n",
    "fprt, tprt, _ = roc_curve(y_train, y_train_hat)\n",
    "auc_rocv = auc(fprv, tprv)\n",
    "auc_roct = auc(fprt, tprt)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC+AUC\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\", label=\"Random, AUC = 0.5\")\n",
    "plt.plot([0, 0], [0, 1], color=\"navy\", linestyle=\":\", label=\"Wizard, AUC = 1.0\")\n",
    "plt.plot([0, 1], [1, 1], color=\"navy\", linestyle=\":\")\n",
    "\n",
    "plt.plot(fprt, tprt, color=\"orange\", label=\"Model - train, AUC = %0.2f\" % auc_roct)\n",
    "plt.plot(fprv, tprv, color=\"red\", label=\"Model - val, AUC = %0.2f\" % auc_rocv)\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Iris dataset from https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv to 'iris' DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `species` column to have value 1 if iris is from _versicolor_ species and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species = np.where(iris.species == \"versicolor\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict `species` column. Split dataset to train and validation subsets using `train_test_split` function. Training set should have **75%** of all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop(\"species\", axis=1)\n",
    "y = iris[\"species\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build logistic regression (with `LogisticRegression` from `sklearn`) using **Elastic-net** regularization with 0.35 L1 ratio (only one solver supports that, check [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "You can read more about **Elastic-net** [here](https://en.wikipedia.org/wiki/Elastic_net_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    penalty=\"elasticnet\", l1_ratio=0.35, solver=\"saga\", max_iter=10000\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction on test set with 0.5 cutoff thresholds. Produce classification report with `classification_report`. What is accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why accuracy is so low? If you want to know check [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Unsupervised-learning-example:-Iris-dimensionality) below `In[19]`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
