{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4 - Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier as CART\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use IMDB 5000 Movies dataset in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"IMDB.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data profiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive data reports for Pandas DataFrames may be generated using [`ydata-profiling`](https://github.com/ydataai/ydata-profiling) package - **IMDB_Report.zip** contains profiling report for IMDB dataset.\n",
    "```\n",
    "profile = ProfileReport(dataset, title=\"IMDB Movies Profiling Report\")\n",
    "profile.to_file(output_file=\"IMDB_Report.html\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial EDA (Exploratory Data Analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking categorical columns\n",
    "dataset.describe(include=[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with high cardinality and imbalanced classes\n",
    "dataset.drop(\n",
    "    [\n",
    "        \"color\",\n",
    "        \"director_name\",\n",
    "        \"actor_2_name\",\n",
    "        \"actor_1_name\",\n",
    "        \"movie_title\",\n",
    "        \"actor_3_name\",\n",
    "        \"plot_keywords\",\n",
    "        \"movie_imdb_link\",\n",
    "        \"language\",\n",
    "        \"country\",\n",
    "        \"content_rating\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "print(dataset.shape)\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping missing values\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_dataset = dataset.select_dtypes(np.number)\n",
    "f, axes = plt.subplots(4, 4, figsize=[15, 15])\n",
    "plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)\n",
    "for n, col in enumerate(numeric_dataset):\n",
    "    sns.regplot(x=col, y=\"imdb_score\", data=dataset, ax=axes[n // 4, n % 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting outliers**\n",
    "\n",
    "A raw score x is converted into a standard score by\n",
    "\n",
    "$$ z= \\frac{x-\\mu}{\\sigma}  $$\n",
    "\n",
    "where:\n",
    "\n",
    "* μ is the mean of the population,\n",
    "* σ is the standard deviation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.zscore(numeric_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "dataset = dataset[(np.abs(stats.zscore(numeric_dataset)) < 9).all(axis=1)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting genres column values\n",
    "dataset[\"genres\"] = dataset.genres.str.split(\"|\")\n",
    "dataset[\"genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distinct categories\n",
    "categories = set(dataset.genres.explode())\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode each movie's classification\n",
    "for cat in categories:\n",
    "    dataset[cat] = dataset.genres.apply(lambda s: int(cat in s))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop genres column\n",
    "dataset.drop(\"genres\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA on cleaned data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize=[15, 15])\n",
    "plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)\n",
    "for n, col in enumerate(dataset.columns[0:16]):\n",
    "    sns.regplot(x=col, y=\"imdb_score\", data=dataset, ax=axes[n // 4, n % 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(6, 4, figsize=[15, 15])\n",
    "plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)\n",
    "for n, col in enumerate(dataset.columns[16:]):\n",
    "    sns.barplot(x=col, y=\"imdb_score\", data=dataset, ax=axes[n // 4, n % 4]).set_ylim(\n",
    "        5,\n",
    "    )\n",
    "f.delaxes(axes[5, 2])\n",
    "f.delaxes(axes[5, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Switching from regression to binary classification task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(dataset.imdb_score))\n",
    "dataset[\"imdb_score\"] = np.where(dataset[\"imdb_score\"] >= 6.0, 1, 0)\n",
    "dataset[\"imdb_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(\"imdb_score\", axis=1)\n",
    "y = dataset[\"imdb_score\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: [sklearn docs](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)\n",
    "\n",
    "**ID3** (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "**C4.5** is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "**C5.0** is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
    "\n",
    "**CART** (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "\n",
    "**scikit-learn uses an optimised version of the CART algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART algorithm pick variables and cutoff threshold using:\n",
    " 1. __for classification__: minimization of node's heterogeneity (Gini index or entropy) \n",
    " 2. __for regression__: minimizing error of prediction (e.g. sum of squares of residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CART, X_val_CART, y_train_CART, y_val_CART = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tree = CART(random_state=42, ccp_alpha=0.0).fit(X_train_CART, y_train_CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Pruning CART tree (cost based)**](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = imdb_tree.cost_complexity_pruning_path(X_train_CART, y_train_CART)\n",
    "ccp_alphas, impurities = path.ccp_alphas[::4], path.impurities[::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_trees = []\n",
    "for cp in ccp_alphas:\n",
    "    imdb_trees.append(\n",
    "        CART(random_state=42, ccp_alpha=cp).fit(X_train_CART, y_train_CART)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order, ind in [(\"First\", 0), (\"Last\", -1)]:\n",
    "    print(\n",
    "        f\"{order} tree with ccp_alpha: {ccp_alphas[ind]:.5f}, \"\n",
    "        + f\"nodes: {imdb_trees[ind].tree_.node_count}, leaves: {imdb_trees[ind].get_n_leaves()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = [accuracy_score(y_val_CART, tree.predict(X_val_CART)) for tree in imdb_trees]\n",
    "train_scores = [accuracy_score(y_train_CART, tree.predict(X_train_CART)) for tree in imdb_trees]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, train_scores, label=\"train\", drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas, val_scores, label=\"validation\", drawstyle=\"steps-post\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs complexity parameter for training and validation sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity (cost) that produce the best tree\n",
    "Best_CART = imdb_trees[np.argmax(val_scores)]\n",
    "Best_CART.ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "_ = plot_tree(Best_CART, feature_names=X_train.columns, filled=True)  # imdb_trees[-1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the best tree\n",
    "round(max(val_scores) * 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble tree-based methods\n",
    "\n",
    "[Ensemble learning](https://scikit-learn.org/stable/modules/ensemble.html) helps improve final model performance by combining results of underlying models (e.g. random forest is combination of decision trees).\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "- In **averaging methods**, the main principle is to build several estimators **independently** and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "> Example: Random forests\n",
    "\n",
    "- By contrast, in **boosting methods**, base estimators are built **sequentially** and the following models tries to reduce the error of the combined estimator.\n",
    "\n",
    "> Example: Boosted trees\n",
    "\n",
    "<img src=\"https://hpccsystems.com/wp-content/uploads/2022/09/LearningTrees-1.png\" width=500>\n",
    "\n",
    "[Source](https://hpccsystems.com/resources/learning-trees-a-guide-to-decision-tree-based-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Random forests**](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)\n",
    "\n",
    "Random forest is a collection of 'weak' decision trees providing good performance together.\n",
    "\n",
    "Trees are weakned using multiple techniques:\n",
    "* bootstrap sample, potentially on subset of available data\n",
    "* limiting number of features\n",
    "* no pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning number of trees and number of features\n",
    "grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "    \"max_features\": np.linspace(1, X_train.shape[1], 5).astype(int),\n",
    "}\n",
    "tuning_res_rf = GridSearchCV(\n",
    "    RFC(random_state=42), param_grid=grid, scoring=\"accuracy\", n_jobs=1, cv=3, verbose=2\n",
    ")\n",
    "tuning_res_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = grid[\"n_estimators\"]\n",
    "max_features = grid[\"max_features\"]\n",
    "arr = tuning_res_rf.cv_results_[\"mean_test_score\"].reshape(\n",
    "    len(max_features), len(n_trees)\n",
    ")\n",
    "df = pd.DataFrame(arr, columns=n_trees, index=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.heatmap(df, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "p.set_xlabel(\"n_estimators\")\n",
    "p.set_ylabel(\"# features\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuning_res_rf.best_params_)\n",
    "Best_RF = tuning_res_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "importances = Best_RF.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in Best_RF.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "num_feat = 6\n",
    "plt.figure(figsize=[15, 8])\n",
    "plt.title(\"Feature importances\", fontsize=20)\n",
    "plt.bar(\n",
    "    range(num_feat)[:num_feat],\n",
    "    importances[indices][:num_feat],\n",
    "    color=\"g\",\n",
    "    yerr=std[indices][:num_feat],\n",
    "    align=\"center\",\n",
    ")\n",
    "plt.xticks(range(num_feat)[:num_feat], X_train.columns[indices[:num_feat]])\n",
    "plt.xlim([-1, num_feat])\n",
    "plt.ylabel(\"Impurity reduction\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Gradient Boosted Trees**](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning number of iterations and percentage of bootstrap sample\n",
    "dist = {\"n_estimators\": stats.randint(100, 400), \"subsample\": stats.uniform()}\n",
    "tuning_res_gbc = RandomizedSearchCV(\n",
    "    GBC(random_state=42),\n",
    "    param_distributions=dist,\n",
    "    scoring=\"accuracy\",\n",
    "    n_iter=25,\n",
    "    n_jobs=1,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    ")\n",
    "tuning_res_gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuning_res_gbc.best_params_)\n",
    "Best_GBT = tuning_res_gbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = Best_GBT.feature_importances_\n",
    "# Make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "num_feat = 6\n",
    "\n",
    "plt.figure(figsize=[12, 8])\n",
    "plt.barh(\n",
    "    pos[-num_feat:],\n",
    "    feature_importance[sorted_idx][-num_feat:],\n",
    "    align=\"center\",\n",
    "    alpha=0.75,\n",
    ")\n",
    "plt.yticks(pos[-num_feat:], X_train.columns[sorted_idx][-num_feat:])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing results of Decision Tree, Random Forest and Gradient Boosted Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Best_CART, Best_RF, Best_GBT]\n",
    "accuracies_test = [accuracy_score(y_test, m.predict(X_test)) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.bar(\n",
    "    [\"CART\", \"Random Forest\", \"Gradient Boosted Trees\"],\n",
    "    accuracies_test,\n",
    "    color=[\"red\", \"green\", \"blue\"],\n",
    "    alpha=0.75\n",
    ")\n",
    "plt.bar_label(fig, fmt=\"%.3f\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Gradient Boosted Trees using `GridSearchCV` setup as for Random Forest (with tuning `n_estimators` and `max_features`). Extract best estimator form the results of grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy for produced Boosted Trees on test data. Add the result as 4th bar on the previously produced barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
